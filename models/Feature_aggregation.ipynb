{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f74a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏥 MIMIC-III 30-Day Readmission Prediction Pipeline\n",
      "=================================================================\n",
      "Features included: LOS, ED Visit, Comorbidity Index, Risk Factors, ICD Codes + Additional Clinical Features\n",
      "\n",
      "🏥 Loading MIMIC-III Tables\n",
      "========================================\n",
      "✓ ADMISSIONS.csv: 58,976 rows, 19 cols\n",
      "✓ PATIENTS.csv: 46,520 rows, 8 cols\n",
      "✓ ICUSTAYS.csv: 61,532 rows, 12 cols\n",
      "✓ TRANSFERS.csv: 261,897 rows, 13 cols\n",
      "✓ DIAGNOSES_ICD.csv.gz (gzipped): 651,047 rows, 5 cols\n",
      "✓ D_ICD_DIAGNOSES.csv: 14,567 rows, 4 cols\n",
      "✓ LABEVENTS.csv (sampled): 50,000 rows, 9 cols\n",
      "✓ D_LABITEMS.csv: 753 rows, 6 cols\n",
      "✓ CHARTEVENTS.csv (sampled): 50,000 rows, 15 cols\n",
      "✓ D_ITEMS.csv: 12,487 rows, 10 cols\n",
      "✓ PRESCRIPTIONS.csv: 4,156,450 rows, 19 cols\n",
      "✓ NOTEEVENTS.csv (sampled): 50,000 rows, 11 cols\n",
      "\n",
      "Optional tables:\n",
      "- PROCEDURES_ICD.csv (not found)\n",
      "- D_ICD_PROCEDURES.csv (error): [Errno 13] Permission denied: ...\n",
      "- DRGCODES.csv (not found)\n",
      "- SERVICES.csv (not found)\n",
      "Successfully loaded 12 tables\n",
      "\n",
      "📊 Creating 30-Day Readmission Labels\n",
      "=============================================\n",
      "Excluded in-hospital deaths: 5,854\n",
      "All readmissions cohort: 53,122 discharges\n",
      "Unplanned cohort: 45,613 discharges\n",
      "Final cohort statistics:\n",
      "  • Total discharges: 45,613\n",
      "  • 30-day readmissions: 3,000\n",
      "  • Readmission rate: 6.58%\n",
      "\n",
      "🔧 Engineering Comprehensive Features\n",
      "=============================================\n",
      "Starting with 45,613 admissions...\n",
      "Adding Length of Stay features...\n",
      "  ✓ ICU data: 57,786 admissions with ICU stays\n",
      "  ✓ Hospital LOS - Mean: 10.4 days\n",
      "Adding Emergency Department features...\n",
      "  ⚠ ED features error: cannot subtract DatetimeArray from ndarray\n",
      "Adding Comorbidity Index features...\n",
      "  ✓ Charlson Score - Mean: 0.30\n",
      "Adding ICD-based risk factors...\n",
      "  ✓ Average high-risk conditions per admission: 0.2\n",
      "Adding Demographics and patient characteristics...\n",
      "  ⚠ Demographics error: Overflow in int64 addition\n",
      "Adding Admission and discharge characteristics...\n",
      "  ⚠ Administrative features error: The column label 'HADM_ID' is not unique.\n",
      "Adding Laboratory values (last 48h)...\n",
      "  ✓ Lab features: 28 features\n",
      "Adding Medication features...\n",
      "  ✓ Medication features: 8 features\n",
      "Adding Previous hospitalization history...\n",
      "  ✓ Previous admission history: Mean previous admissions = 0.9\n",
      "Final cleanup and validation...\n",
      "\n",
      "✅ Feature Engineering Complete!\n",
      "Final dataset: 45,613 records, 85 features\n",
      "Readmission rate: 6.58%\n",
      "\n",
      "Feature Categories:\n",
      "  LOS: 5 features\n",
      "  ED: 5 features\n",
      "  Comorbidity: 18 features\n",
      "  Risk Factors: 16 features\n",
      "  Labs: 12 features\n",
      "  Demographics: 3 features\n",
      "  Medications: 5 features\n",
      "  Previous History: 3 features\n",
      "\n",
      "🎯 PIPELINE COMPLETE!\n",
      "==================================================\n",
      "Dataset Statistics:\n",
      "  • Total Records: 45,613\n",
      "  • Total Features: 82\n",
      "  • Readmission Rate: 6.58%\n",
      "  • Missing Data: 3.3%\n",
      "\n",
      "Class Balance:\n",
      "  • No Readmission: 42,613 (93.4%)\n",
      "  • 30-Day Readmission: 3,000 (6.6%)\n",
      "\n",
      "🤖 Preparing Dataset for Machine Learning\n",
      "=============================================\n",
      "Feature Analysis:\n",
      "  • Total Features: 82\n",
      "  • Numerical Features: 78\n",
      "  • Categorical Features: 4\n",
      "  • High missing (>50%): 3 features\n",
      "    Consider dropping: ['PRIMARY_DIAGNOSIS', 'LACTATE_LAST', 'LACTATE_MEAN']\n",
      "\n",
      "📊 Dataset Ready for ML:\n",
      "  • X (features): (45613, 82)\n",
      "  • y (target): (45613,)\n",
      "  • Ready for train/test split and model training!\n",
      "\n",
      "💾 Dataset saved as 'mimic_readmission_features.csv'\n",
      "\n",
      "🚀 Next Steps:\n",
      "1. Handle categorical encoding (pd.get_dummies or LabelEncoder)\n",
      "2. Split into train/test sets (80/20)\n",
      "3. Scale numerical features if needed\n",
      "4. Train models (RandomForest, XGBoost, Logistic Regression)\n",
      "5. Evaluate performance (AUC-ROC, precision, recall)\n",
      "6. Implement 80%+ accuracy target for clinical deployment\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =================================================================\n",
    "\n",
    "def safe_parse_dob(dob_series):\n",
    "    \"\"\"Safely parse DOB with mixed date formats in MIMIC-III\"\"\"\n",
    "    # Try standard format first\n",
    "    parsed = pd.to_datetime(dob_series, errors='coerce', dayfirst=False)\n",
    "    \n",
    "    # For any NaT entries, try with dayfirst=True\n",
    "    mask = parsed.isna()\n",
    "    if mask.any():\n",
    "        parsed.loc[mask] = pd.to_datetime(dob_series[mask], errors='coerce', dayfirst=True)\n",
    "    \n",
    "    return parsed\n",
    "\n",
    "def safe_datetime_convert(series, column_name):\n",
    "    \"\"\"Safely convert datetime columns with error handling\"\"\"\n",
    "    try:\n",
    "        return pd.to_datetime(series, errors='coerce')\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to parse {column_name}: {e}\")\n",
    "        return pd.Series([pd.NaT] * len(series))\n",
    "\n",
    "# =================================================================\n",
    "# STEP 1: LOAD MIMIC-III TABLES\n",
    "# =================================================================\n",
    "\n",
    "def load_mimic_tables_complete():\n",
    "    \"\"\"Load MIMIC-III tables with mixed CSV and GZ formats\"\"\"\n",
    "    \n",
    "    tables = {}\n",
    "    \n",
    "    # Core table files (adjust paths as needed)\n",
    "    table_files = {\n",
    "        'admissions': 'ADMISSIONS.csv',\n",
    "        'patients': 'PATIENTS.csv', \n",
    "        'icustays': 'ICUSTAYS.csv',\n",
    "        'transfers': 'TRANSFERS.csv',\n",
    "        'diagnoses_icd': 'DIAGNOSES_ICD.csv.gz',  # Your gzipped file\n",
    "        'd_icd_diagnoses': 'D_ICD_DIAGNOSES.csv',\n",
    "        'labevents': 'LABEVENTS.csv',\n",
    "        'd_labitems': 'D_LABITEMS.csv',\n",
    "        'chartevents': 'CHARTEVENTS.csv',\n",
    "        'd_items': 'D_ITEMS.csv',\n",
    "        'prescriptions': 'PRESCRIPTIONS.csv',\n",
    "        'noteevents': 'NOTEEVENTS.csv'\n",
    "    }\n",
    "    \n",
    "    # Optional tables\n",
    "    optional_files = {\n",
    "        'procedures_icd': 'PROCEDURES_ICD.csv',\n",
    "        'd_icd_procedures': 'D_ICD_PROCEDURES.csv',\n",
    "        'drgcodes': 'DRGCODES.csv',\n",
    "        'services': 'SERVICES.csv'\n",
    "    }\n",
    "    \n",
    "    print(\" Loading MIMIC-III Tables\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Load core tables\n",
    "    for table_name, filename in table_files.items():\n",
    "        try:\n",
    "            # Handle gzipped files\n",
    "            if filename.endswith('.gz'):\n",
    "                df = pd.read_csv(filename, compression='gzip', low_memory=False)\n",
    "                print(f\"✓ {filename} (gzipped): {df.shape[0]:,} rows, {df.shape[1]} cols\")\n",
    "            else:\n",
    "                # Handle large CSV files\n",
    "                if filename in ['CHARTEVENTS.csv', 'LABEVENTS.csv', 'NOTEEVENTS.csv']:\n",
    "                    df = pd.read_csv(filename, low_memory=False, nrows=50000)  # Limit for performance\n",
    "                    print(f\"✓ {filename} (sampled): {df.shape[0]:,} rows, {df.shape[1]} cols\")\n",
    "                else:\n",
    "                    df = pd.read_csv(filename)\n",
    "                    print(f\"✓ {filename}: {df.shape[0]:,} rows, {df.shape[1]} cols\")\n",
    "            \n",
    "            tables[table_name] = df\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ Missing: {filename}\")\n",
    "        except PermissionError:\n",
    "            print(f\"✗ Permission denied: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading {filename}: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Load optional tables\n",
    "    print(\"\\nOptional tables:\")\n",
    "    for table_name, filename in optional_files.items():\n",
    "        try:\n",
    "            if filename.endswith('.gz'):\n",
    "                df = pd.read_csv(filename, compression='gzip', low_memory=False)\n",
    "            else:\n",
    "                df = pd.read_csv(filename)\n",
    "            \n",
    "            tables[table_name] = df\n",
    "            print(f\"✓ {filename}: {df.shape[0]:,} rows\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"- {filename} (not found)\")\n",
    "        except Exception as e:\n",
    "            print(f\"- {filename} (error): {str(e)[:30]}...\")\n",
    "    \n",
    "    return tables\n",
    "\n",
    "# =================================================================\n",
    "# STEP 2: CREATE READMISSION LABELS\n",
    "# =================================================================\n",
    "\n",
    "def create_readmission_labels(tables):\n",
    "    \"\"\"Create 30-day readmission labels from ADMISSIONS table\"\"\"\n",
    "    \n",
    "    if 'admissions' not in tables:\n",
    "        print(\" ADMISSIONS table required\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nCreating 30-Day Readmission Labels\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    admissions = tables['admissions'].copy()\n",
    "    \n",
    "    # Convert timestamps safely\n",
    "    admissions['ADMITTIME'] = safe_datetime_convert(admissions['ADMITTIME'], 'ADMITTIME')\n",
    "    admissions['DISCHTIME'] = safe_datetime_convert(admissions['DISCHTIME'], 'DISCHTIME')\n",
    "    \n",
    "    # Remove invalid timestamps\n",
    "    valid_times = admissions['ADMITTIME'].notna() & admissions['DISCHTIME'].notna()\n",
    "    admissions = admissions[valid_times].reset_index(drop=True)\n",
    "    \n",
    "    # Sort by patient and admission time\n",
    "    admissions_sorted = admissions.sort_values(['SUBJECT_ID', 'ADMITTIME']).reset_index(drop=True)\n",
    "    \n",
    "    # Get next admission for each patient\n",
    "    admissions_sorted['NEXT_ADMITTIME'] = admissions_sorted.groupby('SUBJECT_ID')['ADMITTIME'].shift(-1)\n",
    "    \n",
    "    # Calculate days to readmission\n",
    "    admissions_sorted['DAYS_TO_READMIT'] = (\n",
    "        admissions_sorted['NEXT_ADMITTIME'] - admissions_sorted['DISCHTIME']\n",
    "    ).dt.total_seconds() / (24 * 3600)\n",
    "    \n",
    "    # Create 30-day readmission label\n",
    "    admissions_sorted['READMIT_30'] = (\n",
    "        (admissions_sorted['DAYS_TO_READMIT'] > 0) & \n",
    "        (admissions_sorted['DAYS_TO_READMIT'] <= 30)\n",
    "    )\n",
    "    \n",
    "    # Exclude in-hospital deaths for clean cohort\n",
    "    if 'HOSPITAL_EXPIRE_FLAG' in admissions_sorted.columns:\n",
    "        clean_cohort = admissions_sorted[\n",
    "            admissions_sorted['HOSPITAL_EXPIRE_FLAG'] == 0\n",
    "        ].reset_index(drop=True)\n",
    "        print(f\"Excluded in-hospital deaths: {(admissions_sorted['HOSPITAL_EXPIRE_FLAG'] == 1).sum():,}\")\n",
    "    else:\n",
    "        clean_cohort = admissions_sorted\n",
    "    \n",
    "    # Optional: exclude elective readmissions (for unplanned readmission modeling)\n",
    "    if 'ADMISSION_TYPE' in clean_cohort.columns:\n",
    "        # Create both versions\n",
    "        all_readmissions = clean_cohort.copy()\n",
    "        unplanned_cohort = clean_cohort[\n",
    "            clean_cohort['ADMISSION_TYPE'] != 'ELECTIVE'\n",
    "        ].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"All readmissions cohort: {len(all_readmissions):,} discharges\")\n",
    "        print(f\"Unplanned cohort: {len(unplanned_cohort):,} discharges\")\n",
    "        cohort = unplanned_cohort  # Use unplanned for modeling\n",
    "    else:\n",
    "        cohort = clean_cohort\n",
    "    \n",
    "    total_discharges = len(cohort)\n",
    "    readmissions = cohort['READMIT_30'].sum()\n",
    "    readmit_rate = readmissions / total_discharges * 100\n",
    "    \n",
    "    print(f\"Final cohort statistics:\")\n",
    "    print(f\"  • Total discharges: {total_discharges:,}\")\n",
    "    print(f\"  • 30-day readmissions: {readmissions:,}\")\n",
    "    print(f\"  • Readmission rate: {readmit_rate:.2f}%\")\n",
    "    \n",
    "    return cohort\n",
    "\n",
    "# =================================================================\n",
    "# STEP 3: COMPREHENSIVE FEATURE ENGINEERING\n",
    "# =================================================================\n",
    "\n",
    "def create_comprehensive_features(tables, cohort):\n",
    "    \"\"\"Create all requested features: LOS, ED, comorbidity, risk factors, ICD codes + additional features\"\"\"\n",
    "    \n",
    "    print(\"\\n Engineering Comprehensive Features\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Start with base cohort\n",
    "    features_df = cohort[[\n",
    "        'HADM_ID', 'SUBJECT_ID', 'ADMITTIME', 'DISCHTIME', 'READMIT_30'\n",
    "    ]].copy()\n",
    "    \n",
    "    print(f\"Starting with {len(features_df):,} admissions...\")\n",
    "    \n",
    "    # ===== 1. LENGTH OF STAY FEATURES =====\n",
    "    print(\"Adding Length of Stay features...\")\n",
    "    \n",
    "    # Hospital LOS\n",
    "    features_df['HOSPITAL_LOS_HOURS'] = (\n",
    "        features_df['DISCHTIME'] - features_df['ADMITTIME']\n",
    "    ).dt.total_seconds() / 3600\n",
    "    \n",
    "    features_df['HOSPITAL_LOS_DAYS'] = features_df['HOSPITAL_LOS_HOURS'] / 24\n",
    "    \n",
    "    # LOS categories for risk stratification\n",
    "    features_df['LOS_CATEGORY'] = pd.cut(\n",
    "        features_df['HOSPITAL_LOS_DAYS'], \n",
    "        bins=[0, 1, 3, 7, 14, float('inf')],\n",
    "        labels=['<1d', '1-3d', '3-7d', '7-14d', '>14d']\n",
    "    )\n",
    "    \n",
    "    # ICU LOS if available\n",
    "    if 'icustays' in tables:\n",
    "        try:\n",
    "            icustays = tables['icustays'].copy()\n",
    "            icustays['INTIME'] = safe_datetime_convert(icustays['INTIME'], 'INTIME')\n",
    "            icustays['OUTTIME'] = safe_datetime_convert(icustays['OUTTIME'], 'OUTTIME')\n",
    "            \n",
    "            # Calculate ICU LOS\n",
    "            icustays['ICU_LOS_HOURS'] = (\n",
    "                icustays['OUTTIME'] - icustays['INTIME']\n",
    "            ).dt.total_seconds() / 3600\n",
    "            \n",
    "            # Aggregate by admission\n",
    "            icu_summary = icustays.groupby('HADM_ID').agg({\n",
    "                'ICU_LOS_HOURS': ['sum', 'max'],\n",
    "                'ICUSTAY_ID': 'count'\n",
    "            }).round(2)\n",
    "            \n",
    "            icu_summary.columns = ['TOTAL_ICU_LOS_HOURS', 'MAX_ICU_STAY_HOURS', 'NUM_ICU_STAYS']\n",
    "            \n",
    "            features_df = features_df.merge(icu_summary, on='HADM_ID', how='left')\n",
    "            features_df[['TOTAL_ICU_LOS_HOURS', 'MAX_ICU_STAY_HOURS', 'NUM_ICU_STAYS']] = \\\n",
    "                features_df[['TOTAL_ICU_LOS_HOURS', 'MAX_ICU_STAY_HOURS', 'NUM_ICU_STAYS']].fillna(0)\n",
    "            \n",
    "            print(f\"  ✓ ICU data: {icu_summary.shape[0]:,} admissions with ICU stays\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ ICU features skipped: {e}\")\n",
    "            features_df['TOTAL_ICU_LOS_HOURS'] = 0\n",
    "            features_df['NUM_ICU_STAYS'] = 0\n",
    "    else:\n",
    "        features_df['TOTAL_ICU_LOS_HOURS'] = 0\n",
    "        features_df['NUM_ICU_STAYS'] = 0\n",
    "    \n",
    "    print(f\"  ✓ Hospital LOS - Mean: {features_df['HOSPITAL_LOS_DAYS'].mean():.1f} days\")\n",
    "    \n",
    "    # ===== 2. EMERGENCY DEPARTMENT FEATURES =====\n",
    "    print(\"Adding Emergency Department features...\")\n",
    "    \n",
    "    try:\n",
    "        admissions = tables['admissions']\n",
    "        \n",
    "        if 'EDREGTIME' in admissions.columns and 'EDOUTTIME' in admissions.columns:\n",
    "            ed_data = admissions[['HADM_ID', 'EDREGTIME', 'EDOUTTIME', 'ADMITTIME']].copy()\n",
    "            ed_data['EDREGTIME'] = safe_datetime_convert(ed_data['EDREGTIME'], 'EDREGTIME')\n",
    "            ed_data['EDOUTTIME'] = safe_datetime_convert(ed_data['EDOUTTIME'], 'EDOUTTIME')\n",
    "            \n",
    "            # ED indicators and timing\n",
    "            ed_data['HAD_ED_VISIT'] = ed_data['EDREGTIME'].notna().astype(int)\n",
    "            ed_data['ED_LOS_HOURS'] = (ed_data['EDOUTTIME'] - ed_data['EDREGTIME']).dt.total_seconds() / 3600\n",
    "            ed_data['ED_TO_ADMIT_HOURS'] = (ed_data['ADMITTIME'] - ed_data['EDOUTTIME']).dt.total_seconds() / 3600\n",
    "            \n",
    "            # Merge ED features\n",
    "            features_df = features_df.merge(\n",
    "                ed_data[['HADM_ID', 'HAD_ED_VISIT', 'ED_LOS_HOURS', 'ED_TO_ADMIT_HOURS']], \n",
    "                on='HADM_ID', how='left'\n",
    "            )\n",
    "            features_df[['HAD_ED_VISIT', 'ED_LOS_HOURS', 'ED_TO_ADMIT_HOURS']] = \\\n",
    "                features_df[['HAD_ED_VISIT', 'ED_LOS_HOURS', 'ED_TO_ADMIT_HOURS']].fillna(0)\n",
    "            \n",
    "            ed_visits = features_df['HAD_ED_VISIT'].sum()\n",
    "            print(f\"  ✓ ED visits: {ed_visits:,} ({ed_visits/len(features_df)*100:.1f}%)\")\n",
    "        else:\n",
    "            features_df['HAD_ED_VISIT'] = 0\n",
    "            features_df['ED_LOS_HOURS'] = 0\n",
    "            features_df['ED_TO_ADMIT_HOURS'] = 0\n",
    "            print(\"  ⚠ ED timing data not available\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ ED features error: {e}\")\n",
    "        features_df['HAD_ED_VISIT'] = 0\n",
    "        features_df['ED_LOS_HOURS'] = 0\n",
    "        features_df['ED_TO_ADMIT_HOURS'] = 0\n",
    "    \n",
    "    # ===== 3. COMORBIDITY INDICES =====\n",
    "    print(\"Adding Comorbidity Index features...\")\n",
    "    \n",
    "    if 'diagnoses_icd' in tables:\n",
    "        try:\n",
    "            diagnoses = tables['diagnoses_icd']\n",
    "            \n",
    "            # Charlson Comorbidity Index conditions (ICD-9 codes)\n",
    "            charlson_conditions = {\n",
    "                'MI': ['410', '412'],                    # Myocardial infarction\n",
    "                'CHF': ['428'],                          # Congestive heart failure  \n",
    "                'PVD': ['441', '443', '785.4'],         # Peripheral vascular disease\n",
    "                'CVD': ['430', '431', '432', '433', '434', '435', '436', '437', '438'], # Cerebrovascular disease\n",
    "                'DEMENTIA': ['290'],                     # Dementia\n",
    "                'COPD': ['490', '491', '492', '493', '494', '495', '496'], # Chronic pulmonary disease\n",
    "                'RHEUM': ['710.0', '710.1', '710.4', '714.0', '714.1', '714.2'], # Rheumatologic disease\n",
    "                'PUD': ['531', '532', '533', '534'],     # Peptic ulcer disease\n",
    "                'LIVER_MILD': ['571.2', '571.5', '571.6'], # Mild liver disease\n",
    "                'DM': ['250.0', '250.1', '250.2', '250.3'], # Diabetes without complications\n",
    "                'DM_COMP': ['250.4', '250.5', '250.6', '250.7'], # Diabetes with complications\n",
    "                'PLEGIA': ['344.1', '342', '343'],       # Hemi/paraplegia\n",
    "                'RENAL': ['582', '583', '585', '586', '588'], # Renal disease\n",
    "                'CANCER': ['140', '141', '142', '143', '144', '145', '146', '147', '148', '149', \n",
    "                          '150', '151', '152', '153', '154', '155', '156', '157', '158', '159',\n",
    "                          '160', '161', '162', '163', '164', '165', '170', '171', '172', '174',\n",
    "                          '175', '176', '179', '180', '181', '182', '183', '184', '185', '186',\n",
    "                          '187', '188', '189', '190', '191', '192', '193', '194', '195'], # Cancer\n",
    "                'LIVER_SEV': ['572.2', '572.3', '572.4'], # Severe liver disease\n",
    "                'METS': ['196', '197', '198', '199'],    # Metastatic cancer\n",
    "                'AIDS': ['042', '043', '044']            # AIDS\n",
    "            }\n",
    "            \n",
    "            # Calculate Charlson scores (sample for performance)\n",
    "            sample_size = min(10000, len(features_df))\n",
    "            sample_hadm = features_df['HADM_ID'].sample(sample_size).tolist()\n",
    "            \n",
    "            comorbidity_scores = []\n",
    "            \n",
    "            for hadm_id in sample_hadm:\n",
    "                hadm_diagnoses = diagnoses[\n",
    "                    diagnoses['HADM_ID'] == hadm_id\n",
    "                ]['ICD9_CODE'].astype(str)\n",
    "                \n",
    "                charlson_score = 0\n",
    "                condition_flags = {}\n",
    "                \n",
    "                for condition, codes in charlson_conditions.items():\n",
    "                    has_condition = any(hadm_diagnoses.str.startswith(tuple(codes)))\n",
    "                    condition_flags[f'CHARLSON_{condition}'] = int(has_condition)\n",
    "                    \n",
    "                    # Weighted scoring\n",
    "                    if condition in ['MI', 'CHF', 'PVD', 'CVD', 'DEMENTIA', 'COPD', 'RHEUM', 'PUD', 'LIVER_MILD', 'DM']:\n",
    "                        charlson_score += 1 * has_condition\n",
    "                    elif condition in ['DM_COMP', 'PLEGIA', 'RENAL', 'CANCER']:\n",
    "                        charlson_score += 2 * has_condition\n",
    "                    elif condition in ['LIVER_SEV']:\n",
    "                        charlson_score += 3 * has_condition\n",
    "                    elif condition in ['METS', 'AIDS']:\n",
    "                        charlson_score += 6 * has_condition\n",
    "                \n",
    "                record = {\n",
    "                    'HADM_ID': hadm_id, \n",
    "                    'CHARLSON_SCORE': charlson_score,\n",
    "                    'TOTAL_DIAGNOSES': len(hadm_diagnoses)\n",
    "                }\n",
    "                record.update(condition_flags)\n",
    "                comorbidity_scores.append(record)\n",
    "            \n",
    "            # Merge comorbidity features\n",
    "            if comorbidity_scores:\n",
    "                comorbidity_df = pd.DataFrame(comorbidity_scores)\n",
    "                features_df = features_df.merge(comorbidity_df, on='HADM_ID', how='left')\n",
    "                \n",
    "                # Fill missing values for admissions not in sample\n",
    "                charlson_cols = [col for col in comorbidity_df.columns if col != 'HADM_ID']\n",
    "                features_df[charlson_cols] = features_df[charlson_cols].fillna(0)\n",
    "                \n",
    "                print(f\"  ✓ Charlson Score - Mean: {features_df['CHARLSON_SCORE'].mean():.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ Comorbidity features error: {e}\")\n",
    "            features_df['CHARLSON_SCORE'] = 0\n",
    "            features_df['TOTAL_DIAGNOSES'] = 0\n",
    "    else:\n",
    "        features_df['CHARLSON_SCORE'] = 0\n",
    "        features_df['TOTAL_DIAGNOSES'] = 0\n",
    "    \n",
    "    # ===== 4. ICD-BASED RISK FACTORS =====\n",
    "    print(\"Adding ICD-based risk factors...\")\n",
    "    \n",
    "    if 'diagnoses_icd' in tables:\n",
    "        try:\n",
    "            # High-risk diagnosis categories for readmission\n",
    "            high_risk_categories = {\n",
    "                'HEART_FAILURE': ['428'],\n",
    "                'PNEUMONIA': ['480', '481', '482', '483', '484', '485', '486', '487'],\n",
    "                'AMI': ['410'],\n",
    "                'COPD_EXACERB': ['491.21', '491.22', '493.22'],\n",
    "                'SEPSIS': ['995.91', '995.92', '038'],\n",
    "                'STROKE': ['430', '431', '432', '433', '434'],\n",
    "                'RENAL_FAILURE': ['584', '585', '586'],\n",
    "                'GI_BLEED': ['578'],\n",
    "                'DIABETES_COMP': ['250.1', '250.2', '250.3'],\n",
    "                'PSYCHIATRIC': ['295', '296', '297', '298', '300']\n",
    "            }\n",
    "            \n",
    "            # Use same sample as comorbidity calculation\n",
    "            risk_factors = []\n",
    "            \n",
    "            for hadm_id in sample_hadm:\n",
    "                hadm_diagnoses = diagnoses[\n",
    "                    diagnoses['HADM_ID'] == hadm_id\n",
    "                ]['ICD9_CODE'].astype(str)\n",
    "                \n",
    "                risk_record = {'HADM_ID': hadm_id}\n",
    "                \n",
    "                for category, codes in high_risk_categories.items():\n",
    "                    has_condition = any(hadm_diagnoses.str.startswith(tuple(codes)))\n",
    "                    risk_record[f'HAS_{category}'] = int(has_condition)\n",
    "                \n",
    "                # Additional risk indicators\n",
    "                risk_record['PRIMARY_DIAGNOSIS'] = hadm_diagnoses.iloc[0] if len(hadm_diagnoses) > 0 else ''\n",
    "                risk_record['NUM_SECONDARY_DIAGNOSES'] = max(0, len(hadm_diagnoses) - 1)\n",
    "                \n",
    "                risk_factors.append(risk_record)\n",
    "            \n",
    "            # Merge risk factors\n",
    "            if risk_factors:\n",
    "                risk_df = pd.DataFrame(risk_factors)\n",
    "                features_df = features_df.merge(risk_df, on='HADM_ID', how='left')\n",
    "                \n",
    "                # Fill missing values\n",
    "                risk_cols = [col for col in risk_df.columns if col.startswith('HAS_') or col.startswith('NUM_')]\n",
    "                features_df[risk_cols] = features_df[risk_cols].fillna(0)\n",
    "                \n",
    "                high_risk_count = features_df[[col for col in features_df.columns if col.startswith('HAS_')]].sum(axis=1).mean()\n",
    "                print(f\"  ✓ Average high-risk conditions per admission: {high_risk_count:.1f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ Risk factors error: {e}\")\n",
    "    \n",
    "    # ===== 5. DEMOGRAPHICS AND PATIENT CHARACTERISTICS =====\n",
    "    print(\"Adding Demographics and patient characteristics...\")\n",
    "    \n",
    "    if 'patients' in tables:\n",
    "        try:\n",
    "            patients = tables['patients'].copy()\n",
    "            \n",
    "            # Handle DOB with fixed parsing\n",
    "            if 'DOB' in patients.columns:\n",
    "                patients['DOB'] = safe_parse_dob(patients['DOB'])\n",
    "                \n",
    "                # Calculate age at admission\n",
    "                demo_with_admit = patients.merge(\n",
    "                    features_df[['HADM_ID', 'SUBJECT_ID', 'ADMITTIME']], \n",
    "                    on='SUBJECT_ID'\n",
    "                )\n",
    "                \n",
    "                demo_with_admit['AGE_AT_ADMISSION'] = (\n",
    "                    demo_with_admit['ADMITTIME'] - demo_with_admit['DOB']\n",
    "                ).dt.days / 365.25\n",
    "                \n",
    "                # Handle MIMIC age shifting (>89 years old patients have ages >300)\n",
    "                demo_with_admit['AGE_AT_ADMISSION'] = demo_with_admit['AGE_AT_ADMISSION'].apply(\n",
    "                    lambda x: 91.4 if x > 200 else x  # Average age for >89 cohort\n",
    "                )\n",
    "                \n",
    "                # Clean age values\n",
    "                demo_with_admit['AGE_AT_ADMISSION'] = demo_with_admit['AGE_AT_ADMISSION'].fillna(65)\n",
    "                demo_with_admit['AGE_AT_ADMISSION'] = demo_with_admit['AGE_AT_ADMISSION'].clip(0, 95)\n",
    "                \n",
    "                demo_features = demo_with_admit[['HADM_ID', 'GENDER', 'AGE_AT_ADMISSION']]\n",
    "            else:\n",
    "                demo_features = patients[['SUBJECT_ID', 'GENDER']].merge(\n",
    "                    features_df[['HADM_ID', 'SUBJECT_ID']], on='SUBJECT_ID'\n",
    "                )\n",
    "                demo_features['AGE_AT_ADMISSION'] = 65  # Default age\n",
    "            \n",
    "            # Add death information if available\n",
    "            if 'DOD' in patients.columns:\n",
    "                patients['DOD'] = safe_datetime_convert(patients['DOD'], 'DOD')\n",
    "                demo_features = demo_features.merge(\n",
    "                    patients[['SUBJECT_ID', 'DOD']], on='SUBJECT_ID', how='left'\n",
    "                )\n",
    "                \n",
    "                # Calculate mortality within 1 year (outcome predictor)\n",
    "                demo_features = demo_features.merge(\n",
    "                    features_df[['HADM_ID', 'DISCHTIME']], on='HADM_ID'\n",
    "                )\n",
    "                demo_features['DIED_WITHIN_1_YEAR'] = (\n",
    "                    (demo_features['DOD'] - demo_features['DISCHTIME']).dt.days <= 365\n",
    "                ).fillna(False).astype(int)\n",
    "                demo_features = demo_features.drop(['DOD', 'DISCHTIME'], axis=1)\n",
    "            \n",
    "            features_df = features_df.merge(demo_features, on='HADM_ID', how='left')\n",
    "            \n",
    "            # Fill missing demographics\n",
    "            features_df['GENDER'] = features_df['GENDER'].fillna('U')\n",
    "            features_df['AGE_AT_ADMISSION'] = features_df['AGE_AT_ADMISSION'].fillna(65)\n",
    "            \n",
    "            # Age categories\n",
    "            features_df['AGE_GROUP'] = pd.cut(\n",
    "                features_df['AGE_AT_ADMISSION'],\n",
    "                bins=[0, 18, 35, 50, 65, 80, 100],\n",
    "                labels=['<18', '18-35', '35-50', '50-65', '65-80', '80+']\n",
    "            )\n",
    "            \n",
    "            print(f\"  ✓ Demographics - Mean age: {features_df['AGE_AT_ADMISSION'].mean():.1f} years\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ Demographics error: {e}\")\n",
    "            features_df['GENDER'] = 'U'\n",
    "            features_df['AGE_AT_ADMISSION'] = 65\n",
    "            features_df['AGE_GROUP'] = '50-65'\n",
    "    \n",
    "    # ===== 6. ADMISSION AND DISCHARGE CHARACTERISTICS =====\n",
    "    print(\"Adding Admission and discharge characteristics...\")\n",
    "    \n",
    "    try:\n",
    "        admissions = tables['admissions']\n",
    "        admin_cols = ['HADM_ID', 'ADMISSION_TYPE', 'ADMISSION_LOCATION', \n",
    "                     'DISCHARGE_LOCATION', 'INSURANCE', 'LANGUAGE', 'RELIGION', 'MARITAL_STATUS']\n",
    "        \n",
    "        # Select available columns\n",
    "        available_cols = ['HADM_ID'] + [col for col in admin_cols if col in admissions.columns]\n",
    "        admin_features = admissions[available_cols]\n",
    "        \n",
    "        features_df = features_df.merge(admin_features, on='HADM_ID', how='left')\n",
    "        \n",
    "        # Fill missing values\n",
    "        categorical_cols = [col for col in available_cols if col != 'HADM_ID']\n",
    "        for col in categorical_cols:\n",
    "            if col in features_df.columns:\n",
    "                features_df[col] = features_df[col].fillna('UNKNOWN')\n",
    "        \n",
    "        print(f\"  ✓ Administrative features: {len(available_cols)-1} columns\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ Administrative features error: {e}\")\n",
    "    \n",
    "    # ===== 7. LAB VALUES (LAST 48H BEFORE DISCHARGE) =====\n",
    "    print(\"Adding Laboratory values (last 48h)...\")\n",
    "    \n",
    "    if 'labevents' in tables and 'd_labitems' in tables:\n",
    "        try:\n",
    "            labevents = tables['labevents'].copy()\n",
    "            d_labitems = tables['d_labitems']\n",
    "            \n",
    "            # Convert lab time\n",
    "            labevents['CHARTTIME'] = safe_datetime_convert(labevents['CHARTTIME'], 'CHARTTIME')\n",
    "            \n",
    "            # Key lab tests for readmission risk\n",
    "            key_lab_itemids = {\n",
    "                'HEMOGLOBIN': [51222, 51248, 51249],\n",
    "                'CREATININE': [50912, 51081],\n",
    "                'BUN': [51006],\n",
    "                'GLUCOSE': [50809, 50931],\n",
    "                'SODIUM': [50824, 50983],\n",
    "                'POTASSIUM': [50822, 50971],\n",
    "                'WBC': [51300, 51301],\n",
    "                'PLATELET': [51265],\n",
    "                'LACTATE': [50813]\n",
    "            }\n",
    "            \n",
    "            # Sample admissions for performance\n",
    "            sample_for_labs = features_df['HADM_ID'].sample(min(2000, len(features_df))).tolist()\n",
    "            \n",
    "            lab_features = []\n",
    "            \n",
    "            for hadm_id in sample_for_labs:\n",
    "                dischtime = features_df[features_df['HADM_ID'] == hadm_id]['DISCHTIME'].iloc[0]\n",
    "                window_start = dischtime - timedelta(hours=48)\n",
    "                \n",
    "                hadm_labs = labevents[\n",
    "                    (labevents['HADM_ID'] == hadm_id) &\n",
    "                    (labevents['CHARTTIME'] >= window_start) &\n",
    "                    (labevents['CHARTTIME'] <= dischtime)\n",
    "                ]\n",
    "                \n",
    "                lab_record = {'HADM_ID': hadm_id}\n",
    "                \n",
    "                for lab_name, item_ids in key_lab_itemids.items():\n",
    "                    lab_values = hadm_labs[\n",
    "                        hadm_labs['ITEMID'].isin(item_ids)\n",
    "                    ]['VALUENUM'].dropna()\n",
    "                    \n",
    "                    if len(lab_values) > 0:\n",
    "                        lab_record[f'{lab_name}_LAST'] = lab_values.iloc[-1]\n",
    "                        lab_record[f'{lab_name}_MEAN'] = lab_values.mean()\n",
    "                        lab_record[f'{lab_name}_ABNORMAL'] = int(\n",
    "                            lab_values.iloc[-1] > lab_values.quantile(0.95) or\n",
    "                            lab_values.iloc[-1] < lab_values.quantile(0.05)\n",
    "                        )\n",
    "                    else:\n",
    "                        lab_record[f'{lab_name}_LAST'] = None\n",
    "                        lab_record[f'{lab_name}_MEAN'] = None\n",
    "                        lab_record[f'{lab_name}_ABNORMAL'] = 0\n",
    "                \n",
    "                lab_record['TOTAL_LAB_TESTS'] = len(hadm_labs)\n",
    "                lab_features.append(lab_record)\n",
    "            \n",
    "            if lab_features:\n",
    "                lab_df = pd.DataFrame(lab_features)\n",
    "                features_df = features_df.merge(lab_df, on='HADM_ID', how='left')\n",
    "                \n",
    "                # Fill missing lab values\n",
    "                lab_cols = [col for col in lab_df.columns if col != 'HADM_ID']\n",
    "                for col in lab_cols:\n",
    "                    if col.endswith('_ABNORMAL') or col == 'TOTAL_LAB_TESTS':\n",
    "                        features_df[col] = features_df[col].fillna(0)\n",
    "                    else:\n",
    "                        features_df[col] = features_df[col].fillna(features_df[col].median())\n",
    "                \n",
    "                print(f\"  ✓ Lab features: {len([col for col in lab_df.columns if col != 'HADM_ID'])} features\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ Lab features error: {e}\")\n",
    "    \n",
    "    # ===== 8. MEDICATION FEATURES =====\n",
    "    print(\"Adding Medication features...\")\n",
    "    \n",
    "    if 'prescriptions' in tables:\n",
    "        try:\n",
    "            prescriptions = tables['prescriptions']\n",
    "            \n",
    "            # High-risk medication categories\n",
    "            high_risk_meds = {\n",
    "                'ANTICOAGULANTS': ['warfarin', 'heparin', 'enoxaparin', 'coumadin'],\n",
    "                'DIURETICS': ['furosemide', 'lasix', 'hydrochlorothiazide'],\n",
    "                'INSULIN': ['insulin'],\n",
    "                'OPIOIDS': ['morphine', 'fentanyl', 'oxycodone', 'hydrocodone'],\n",
    "                'STEROIDS': ['prednisone', 'methylprednisolone', 'hydrocortisone'],\n",
    "                'ANTIBIOTICS': ['vancomycin', 'ciprofloxacin', 'levofloxacin']\n",
    "            }\n",
    "            \n",
    "            med_features = []\n",
    "            \n",
    "            # Sample for performance\n",
    "            sample_for_meds = features_df['HADM_ID'].sample(min(5000, len(features_df))).tolist()\n",
    "            \n",
    "            for hadm_id in sample_for_meds:\n",
    "                hadm_meds = prescriptions[\n",
    "                    prescriptions['HADM_ID'] == hadm_id\n",
    "                ]['DRUG'].str.lower()\n",
    "                \n",
    "                med_record = {'HADM_ID': hadm_id}\n",
    "                med_record['TOTAL_MEDICATIONS'] = len(hadm_meds)\n",
    "                \n",
    "                for med_category, med_list in high_risk_meds.items():\n",
    "                    has_med = any(\n",
    "                        hadm_meds.str.contains('|'.join(med_list), na=False)\n",
    "                    )\n",
    "                    med_record[f'HAS_{med_category}'] = int(has_med)\n",
    "                \n",
    "                # Polypharmacy indicator\n",
    "                med_record['POLYPHARMACY'] = int(len(hadm_meds) >= 10)\n",
    "                \n",
    "                med_features.append(med_record)\n",
    "            \n",
    "            if med_features:\n",
    "                med_df = pd.DataFrame(med_features)\n",
    "                features_df = features_df.merge(med_df, on='HADM_ID', how='left')\n",
    "                \n",
    "                # Fill missing medication features\n",
    "                med_cols = [col for col in med_df.columns if col != 'HADM_ID']\n",
    "                features_df[med_cols] = features_df[med_cols].fillna(0)\n",
    "                \n",
    "                print(f\"  ✓ Medication features: {len(med_cols)} features\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ Medication features error: {e}\")\n",
    "    \n",
    "    # ===== 9. PREVIOUS HOSPITALIZATIONS =====\n",
    "    print(\"Adding Previous hospitalization history...\")\n",
    "    \n",
    "    try:\n",
    "        # Count previous admissions for each patient\n",
    "        prev_admissions = cohort.groupby('SUBJECT_ID').agg({\n",
    "            'HADM_ID': 'count',\n",
    "            'ADMITTIME': ['min', 'max']\n",
    "        }).reset_index()\n",
    "        \n",
    "        prev_admissions.columns = ['SUBJECT_ID', 'TOTAL_ADMISSIONS', 'FIRST_ADMISSION', 'LAST_ADMISSION']\n",
    "        \n",
    "        # Calculate days since last admission\n",
    "        current_features = features_df[['HADM_ID', 'SUBJECT_ID', 'ADMITTIME']].merge(\n",
    "            prev_admissions, on='SUBJECT_ID'\n",
    "        )\n",
    "        \n",
    "        current_features['DAYS_SINCE_LAST_ADMISSION'] = (\n",
    "            current_features['ADMITTIME'] - current_features['LAST_ADMISSION']\n",
    "        ).dt.days\n",
    "        \n",
    "        current_features['PREVIOUS_ADMISSIONS'] = current_features['TOTAL_ADMISSIONS'] - 1\n",
    "        current_features['FREQUENT_FLYER'] = (current_features['PREVIOUS_ADMISSIONS'] >= 3).astype(int)\n",
    "        \n",
    "        hist_features = current_features[['HADM_ID', 'PREVIOUS_ADMISSIONS', 'FREQUENT_FLYER', 'DAYS_SINCE_LAST_ADMISSION']]\n",
    "        features_df = features_df.merge(hist_features, on='HADM_ID', how='left')\n",
    "        \n",
    "        # Fill missing values\n",
    "        features_df['PREVIOUS_ADMISSIONS'] = features_df['PREVIOUS_ADMISSIONS'].fillna(0)\n",
    "        features_df['FREQUENT_FLYER'] = features_df['FREQUENT_FLYER'].fillna(0)\n",
    "        features_df['DAYS_SINCE_LAST_ADMISSION'] = features_df['DAYS_SINCE_LAST_ADMISSION'].fillna(365)\n",
    "        \n",
    "        print(f\"  ✓ Previous admission history: Mean previous admissions = {features_df['PREVIOUS_ADMISSIONS'].mean():.1f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ Previous admission features error: {e}\")\n",
    "        features_df['PREVIOUS_ADMISSIONS'] = 0\n",
    "        features_df['FREQUENT_FLYER'] = 0\n",
    "        features_df['DAYS_SINCE_LAST_ADMISSION'] = 365\n",
    "    \n",
    "    # ===== FINAL CLEANUP =====\n",
    "    print(\"Final cleanup and validation...\")\n",
    "    \n",
    "    # Remove timestamp columns\n",
    "    features_df = features_df.drop(['ADMITTIME', 'DISCHTIME'], axis=1, errors='ignore')\n",
    "    \n",
    "    # Convert categorical variables to category dtype for memory efficiency\n",
    "    categorical_columns = [\n",
    "        'LOS_CATEGORY', 'AGE_GROUP', 'GENDER', 'ADMISSION_TYPE', \n",
    "        'ADMISSION_LOCATION', 'DISCHARGE_LOCATION', 'INSURANCE', \n",
    "        'MARITAL_STATUS', 'PRIMARY_DIAGNOSIS'\n",
    "    ]\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col in features_df.columns:\n",
    "            features_df[col] = features_df[col].astype('category')\n",
    "    \n",
    "    # Final statistics\n",
    "    print(f\"\\n✅ Feature Engineering Complete!\")\n",
    "    print(f\"Final dataset: {features_df.shape[0]:,} records, {features_df.shape[1]} features\")\n",
    "    print(f\"Readmission rate: {features_df['READMIT_30'].mean()*100:.2f}%\")\n",
    "    \n",
    "    # Feature summary by category\n",
    "    feature_categories = {\n",
    "        'LOS': [col for col in features_df.columns if 'LOS' in col],\n",
    "        'ED': [col for col in features_df.columns if 'ED' in col],\n",
    "        'Comorbidity': [col for col in features_df.columns if 'CHARLSON' in col],\n",
    "        'Risk Factors': [col for col in features_df.columns if 'HAS_' in col],\n",
    "        'Labs': [col for col in features_df.columns if any(lab in col for lab in ['HEMOGLOBIN', 'CREATININE', 'BUN', 'GLUCOSE'])],\n",
    "        'Demographics': [col for col in features_df.columns if col in ['GENDER', 'AGE_AT_ADMISSION', 'AGE_GROUP']],\n",
    "        'Medications': [col for col in features_df.columns if 'MEDICATION' in col or col.startswith('HAS_') and any(med in col for med in ['ANTICOAG', 'DIURETIC', 'INSULIN', 'OPIOID'])],\n",
    "        'Previous History': [col for col in features_df.columns if 'PREVIOUS' in col or 'FREQUENT' in col or 'DAYS_SINCE' in col]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nFeature Categories:\")\n",
    "    for category, features in feature_categories.items():\n",
    "        if features:\n",
    "            print(f\"  {category}: {len(features)} features\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# =================================================================\n",
    "# STEP 4: MAIN PIPELINE EXECUTION\n",
    "# =================================================================\n",
    "\n",
    "def run_complete_readmission_pipeline():\n",
    "    \"\"\"Complete end-to-end pipeline for MIMIC-III readmission prediction\"\"\"\n",
    "    \n",
    "    print(\"🏥 MIMIC-III 30-Day Readmission Prediction Pipeline\")\n",
    "    print(\"=\" * 65)\n",
    "    print(\"Features included: LOS, ED Visit, Comorbidity Index, Risk Factors, ICD Codes + Additional Clinical Features\")\n",
    "    print(\"\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load tables\n",
    "        tables = load_mimic_tables_complete()\n",
    "        \n",
    "        if not tables:\n",
    "            print(\"No tables loaded - check file paths and permissions\")\n",
    "            return None\n",
    "        \n",
    "        if 'admissions' not in tables:\n",
    "            print(\" ADMISSIONS table required but missing\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Successfully loaded {len(tables)} tables\")\n",
    "        \n",
    "        # Step 2: Create readmission labels\n",
    "        cohort = create_readmission_labels(tables)\n",
    "        if cohort is None:\n",
    "            print(\" Failed to create readmission labels\")\n",
    "            return None\n",
    "        \n",
    "        # Step 3: Engineer comprehensive features\n",
    "        final_dataset = create_comprehensive_features(tables, cohort)\n",
    "        \n",
    "        # Step 4: Final validation and summary\n",
    "        print(f\"\\n PIPELINE COMPLETE!\")\n",
    "        print(f\"=\"*50)\n",
    "        print(f\"Dataset Statistics:\")\n",
    "        print(f\"  • Total Records: {final_dataset.shape[0]:,}\")\n",
    "        print(f\"  • Total Features: {final_dataset.shape[1]-3}\")  # Excluding HADM_ID, SUBJECT_ID, READMIT_30\n",
    "        print(f\"  • Readmission Rate: {final_dataset['READMIT_30'].mean()*100:.2f}%\")\n",
    "        print(f\"  • Missing Data: {(final_dataset.isnull().sum().sum() / (final_dataset.shape[0] * final_dataset.shape[1]) * 100):.1f}%\")\n",
    "        \n",
    "        # Check class balance\n",
    "        readmit_counts = final_dataset['READMIT_30'].value_counts()\n",
    "        print(f\"\\nClass Balance:\")\n",
    "        print(f\"  • No Readmission: {readmit_counts[0]:,} ({readmit_counts[0]/len(final_dataset)*100:.1f}%)\")\n",
    "        print(f\"  • 30-Day Readmission: {readmit_counts[1]:,} ({readmit_counts[1]/len(final_dataset)*100:.1f}%)\")\n",
    "        \n",
    "        return final_dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Pipeline failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# =================================================================\n",
    "# STEP 5: PREPARE FOR MACHINE LEARNING\n",
    "# =================================================================\n",
    "\n",
    "def prepare_ml_dataset(final_dataset):\n",
    "    \"\"\"Prepare the dataset for machine learning models\"\"\"\n",
    "    \n",
    "    if final_dataset is None:\n",
    "        print(\"No dataset to prepare\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\n Preparing Dataset for Machine Learning\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Identify feature columns (exclude IDs and target)\n",
    "    exclude_cols = ['HADM_ID', 'SUBJECT_ID', 'READMIT_30']\n",
    "    feature_columns = [col for col in final_dataset.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = final_dataset[feature_columns].copy()\n",
    "    y = final_dataset['READMIT_30'].copy()\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    categorical_cols = X.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    print(f\"Feature Analysis:\")\n",
    "    print(f\"  • Total Features: {len(feature_columns)}\")\n",
    "    print(f\"  • Numerical Features: {len(numerical_cols)}\")\n",
    "    print(f\"  • Categorical Features: {len(categorical_cols)}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_summary = X.isnull().sum()\n",
    "    high_missing = missing_summary[missing_summary > len(X) * 0.5]\n",
    "    if len(high_missing) > 0:\n",
    "        print(f\"  • High missing (>50%): {len(high_missing)} features\")\n",
    "        print(f\"    Consider dropping: {high_missing.index.tolist()}\")\n",
    "    \n",
    "    print(f\"\\n Dataset Ready :\")\n",
    "    print(f\"  • X (features): {X.shape}\")\n",
    "    print(f\"  • y (target): {y.shape}\")\n",
    "    print(f\"  • Ready for train/test split and model training!\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# =================================================================\n",
    "# MAIN EXECUTION\n",
    "# =================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the complete pipeline\n",
    "    final_dataset = run_complete_readmission_pipeline()\n",
    "    \n",
    "    # Prepare for machine learning\n",
    "    if final_dataset is not None:\n",
    "        X, y = prepare_ml_dataset(final_dataset)\n",
    "        \n",
    "        # Save the dataset\n",
    "        try:\n",
    "            final_dataset.to_csv('mimic_readmission_features.csv', index=False)\n",
    "            print(f\"\\n Dataset saved as 'mimic_readmission_features.csv'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save dataset: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\" Pipeline failed - check your MIMIC-III file setup and try again\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad88028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24c622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97372fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f42930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c731a5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956594e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
